{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "# from trl import \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"<Dataset name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "quantization = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = \"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"PRAli22/arat5-arabic-dialects-translation\", quantization_config = quantization)\n",
    "\n",
    "tokonizer = AutoTokenizer.from_pretrained(\"PRAli22/arat5-arabic-dialects-translation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Dropout\n",
    "\n",
    "new_dropout_rate = 0.4\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Dropout):\n",
    "        module.p = new_dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.dropout_rate = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lora configiration\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        lora_alpha=32,  \n",
    "        lora_dropout=0.50,  \n",
    "        r=18, \n",
    "        bias=\"none\",\n",
    "        task_type=\"Seq2Seq\",\n",
    "        target_modules= \n",
    "         ['k', 'v', 'q', 'o', 'wi_0', 'wi_1', 'wo', 'lm_head']\n",
    "    )\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model_with_lora = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "msa_length = df[\"msa\"].apply(lambda x: len(x.split())).mean()\n",
    "hijazi_length = df[\"hijazi\"].apply(lambda x: len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "msa_length, hijazi_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=50):\n",
    "        self.src = []\n",
    "        self.dest = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            # Add translation pairs\n",
    "            self.src.append(f\"ترجم من حجازي الى فصحى: {df.iloc[i, 2]}\")  # Hijazi to MSA\n",
    "            self.dest.append(df.iloc[i, 0])  # MSA\n",
    "\n",
    "            self.src.append(f\"ترجم من فصحى الى حجازي: {df.iloc[i, 0]}\")  # MSA to Hijazi\n",
    "            self.dest.append(df.iloc[i, 2])  # Hijazi\n",
    "\n",
    "            self.src.append(f\"ترجم من جنوبي الى فصحى: {df.iloc[i, 1]}\")  # Janoubiyah to MSA\n",
    "            self.dest.append(df.iloc[i, 0])  # MSA\n",
    "\n",
    "            self.src.append(f\"ترجم من فصحى الى جنوبي: {df.iloc[i, 0]}\")  # MSA to Janoubiyah\n",
    "            self.dest.append(df.iloc[i, 1])  # Janoubiyah\n",
    "\n",
    "            self.src.append(f\"ترجم من جنوبي الى حجازي: {df.iloc[i, 1]}\")  # Janoubiyah to Hijazi\n",
    "            self.dest.append(df.iloc[i, 2])  # Hijazi\n",
    "\n",
    "            self.src.append(f\"ترجم من حجازي الى جنوبي: {df.iloc[i, 2]}\")  # Hijazi to Janoubiyah\n",
    "            self.dest.append(df.iloc[i, 1])  # Janoubiyah\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize source and destination texts\n",
    "        inputs = self.tokenizer(\n",
    "            self.src[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            self.dest[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df, tokonizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def compute_cost(eval_pred):\n",
    "    preds, trues = eval_pred.predictions, eval_pred.label_ids\n",
    "    pred_ids = preds.tolist()\n",
    "    true_ids = trues.tolist()\n",
    "    \n",
    "    \n",
    "    pred_ids = [\n",
    "            [token if token >= 0 and token < tokonizer.vocab_size else tokonizer.pad_token_id for token in pred_seq]\n",
    "            for pred_seq in pred_ids\n",
    "        ]\n",
    "\n",
    "    preds_text = tokonizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    trues_text = tokonizer.batch_decode(true_ids, skip_special_tokens=True)\n",
    "    result = {}\n",
    "    result[\"bleu\"] = bleu_metric.compute(predictions=preds_text, references=[[t] for t in trues_text])\n",
    "\n",
    "    bleu_score = result[\"bleu\"][\"bleu\"]\n",
    "    writer.add_scalar(tag=\"eval/bleu\", scalar_value=bleu_score)\n",
    "\n",
    "    for idx, precision in enumerate(result[\"bleu\"][\"precisions\"]):\n",
    "        writer.add_scalar(tag=f\"precisions{idx+1}-gram\", scalar_value=precision)\n",
    "    return result[\"bleu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainerarg = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"Chickpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    num_train_epochs = 50,\n",
    "    logging_dir = 'logging/',\n",
    "    logging_steps = 100,\n",
    "    save_steps= 5000,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate = 0.0003,\n",
    "    weight_decay = 0.80,\n",
    "    lr_scheduler_type= \"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "    generation_max_length = 50,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size  = 32,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model_with_lora,\n",
    "    args = trainerarg,\n",
    "    train_dataset = train,\n",
    "    eval_dataset = test,\n",
    "    compute_metrics = compute_cost,\n",
    "    tokenizer=tokonizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6157477,
     "sourceId": 10003226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6164414,
     "sourceId": 10012707,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6172677,
     "sourceId": 10023760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6177510,
     "sourceId": 10030333,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
